{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15377a0ffa5c1347",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# U-Net: For Organoid image segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:23:41.748402500Z",
     "start_time": "2023-11-30T09:23:30.878515900Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Annmariya.sebastian\\PycharmProjects\\calcium-gui\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dfa42f6b28d6d4",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data preparation\n",
    "Dataset images along with  corresponding segmentation masks (ground truth labels) indicating the areas of interest (e.g., boundaries, objects, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "341549ee91fb64c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:23:41.764024Z",
     "start_time": "2023-11-30T09:23:41.748402500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def resize_normalize(img_path, target_size=(128, 128)):\n",
    "    img = Image.open(img_path)\n",
    "    img = img.resize(target_size)  # Resize to the target size\n",
    "    img = np.array(img) / 255.0  # Normalize pixel values (assuming 0-255 range)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc62d4b0d2c63cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:23:41.810894600Z",
     "start_time": "2023-11-30T09:23:41.764024Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def process_dataset(source_folder):\n",
    "    images = []\n",
    "\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        file_path = os.path.join(source_folder, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            image = resize_normalize(file_path)\n",
    "            images.append(image)\n",
    "\n",
    "    images = np.array(images)\n",
    "    print(images.shape)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c522018158b6c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:03.054816900Z",
     "start_time": "2023-11-30T09:23:41.779643800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(544, 128, 128, 3)\n",
      "(544, 128, 128, 3)\n",
      "(2537, 128, 128, 3)\n",
      "(544, 128, 128, 3)\n",
      "(544, 128, 128, 3)\n",
      "(2537, 128, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The data for training contains 3625 512*512 images\n",
    "test_images = process_dataset('dataset/test/')  # 544\n",
    "valid_images = process_dataset('dataset/valid/')  # 544\n",
    "train_images = process_dataset('dataset/train/')  # 2537\n",
    "\n",
    "test_mask_images = process_dataset('dataset/test_mask/')  # 544\n",
    "valid_mask_images = process_dataset('dataset/valid_mask/')  # 544\n",
    "train_mask_images = process_dataset('dataset/train_mask/')  # 2537"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80ee31366af0c4c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## U-Net model\n",
    "\n",
    "Contracting path (Encoder containing down-sampling steps):\n",
    "* Images are first fed through several convolutional layers which reduce height and width, while growing the number of channels.\n",
    "* The contracting path follows a regular CNN architecture, with convolutional layers, their activations, and pooling layers to down-sample the image and extract its features. In detail, it consists of the repeated application of two 3 x 3 same padding convolutions, each followed by a rectified linear unit (ReLU) and a 2 x 2 max pooling operation with stride 2 for down-sampling. At each down-sampling step, the number of feature channels is doubled.\n",
    "* Crop function: This step crops the image from the contracting path and concatenates it to the current image on the expanding path to create a skip connection.\n",
    "\n",
    "Expanding path (Decoder containing up-sampling steps):\n",
    "* The expanding path performs the opposite operation of the contracting path, growing the image back to its original size, while shrinking the channels gradually.\n",
    "* In detail, each step in the expanding path up-samples the feature map, followed by a 2 x 2 convolution (the transposed convolution). This transposed convolution halves the number of feature channels, while growing the height and width of the image.\n",
    "* Next is a concatenation with the correspondingly cropped feature map from the contracting path, and two 3 x 3 convolutions, each followed by a ReLU. You need to perform cropping to handle the loss of border pixels in every convolution.\n",
    "\n",
    "Final Feature Mapping Block:\n",
    "* In the final layer, a 1x1 convolution is used to map each 64-component feature vector to the desired number of classes. The channel dimensions from the previous layer correspond to the number of filters used, so when you use 1x1 convolutions, you can transform that dimension by choosing an appropriate number of 1x1 filters. When this idea is applied to the last layer, you can reduce the channel dimensions to have one layer per class.\n",
    "\n",
    "The U-Net network has 23 convolutional layers in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d0c0a6300ff37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:03.132924100Z",
     "start_time": "2023-11-30T09:26:03.086090600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def double_conv_block(input_image, n_filters):\n",
    "\n",
    "    # Convolution with 3x3 filter followed by ReLU activation\n",
    "    conv_relu_one = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(input_image)\n",
    "    conv_relu_two = layers.Conv2D(n_filters, 3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\")(conv_relu_one)\n",
    "\n",
    "    return conv_relu_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9891008fc96590d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:03.164196300Z",
     "start_time": "2023-11-30T09:26:03.148574600Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def down_sample(input_image, n_filters):\n",
    "    \"\"\"\n",
    "    Encoder\n",
    "    Two 3x3 convolutions with ReLU activation functions, which perform feature extraction,\n",
    "    and a 2x2 max-pooling layer for down-sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    conv_relu_two = double_conv_block(input_image, n_filters)\n",
    "    # Max Pooling with 2x2 filter\n",
    "    max_pool = layers.MaxPool2D(2)(conv_relu_two)\n",
    "    max_pool = layers.Dropout(0.3)(max_pool)\n",
    "\n",
    "    return conv_relu_two, max_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92e29509e1fe395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:03.179818200Z",
     "start_time": "2023-11-30T09:26:03.164196300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def up_sample(input_image, conv_features, n_filters):\n",
    "    \"\"\"\n",
    "    Decoder\n",
    "    Comprises up-convolutions (transposed convolutions) that increase the spatial resolution of the feature maps.\n",
    "    Each step in this path involves an up-convolutional layer followed by concatenation with the corresponding feature maps from the contracting path. This step helps in preserving spatial information and finer details.\n",
    "    After concatenation, there are two 3x3 convolutions with ReLU activation functions to further process the combined features.\n",
    "    \"\"\"\n",
    "    segmentation_map = layers.Conv2DTranspose(n_filters, 3, 2, padding=\"same\")(input_image)\n",
    "    # Copy and crop the skip features\n",
    "    segmentation_map = layers.concatenate([segmentation_map, conv_features])\n",
    "    segmentation_map = layers.Dropout(0.3)(segmentation_map)\n",
    "    segmentation_map = double_conv_block(segmentation_map, n_filters)\n",
    "\n",
    "    return segmentation_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d8733e047495afe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:03.211030400Z",
     "start_time": "2023-11-30T09:26:03.179818200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def build_unet_model():\n",
    "\n",
    "    # inputs\n",
    "    inputs = layers.Input(shape=(128, 128, 3))\n",
    "\n",
    "    # encoder: Contracting path - down sample\n",
    "    conv_relu_two1, max_pool1 = down_sample(inputs, 64)\n",
    "    conv_relu_two2, max_pool2 = down_sample(max_pool1, 128)\n",
    "    conv_relu_two3, max_pool3 = down_sample(max_pool2, 256)\n",
    "    conv_relu_two4, max_pool4 = down_sample(max_pool3, 512)\n",
    "\n",
    "    # 5 - bottleneck\n",
    "    bottleneck = double_conv_block(max_pool4, 1024)\n",
    "\n",
    "    # decoder: Expanding path - up sample\n",
    "    up_conv1 = up_sample(bottleneck, conv_relu_two4, 512)\n",
    "    up_conv2 = up_sample(up_conv1, conv_relu_two3, 256)\n",
    "    up_conv3 = up_sample(up_conv2, conv_relu_two2, 128)\n",
    "    up_conv4 = up_sample(up_conv3, conv_relu_two1, 64)\n",
    "\n",
    "    \"\"\"\n",
    "    A 1x1 convolutional layer with a softmax activation function (for multi-class segmentation) or a \n",
    "    sigmoid activation function (for binary segmentation) is used to produce the final segmentation map.\n",
    "    \"\"\"\n",
    "    # The output has three channels corresponding to the 2 classes that the model will classify each pixel for:\n",
    "    # background, foreground object\n",
    "    segmentation_map = layers.Conv2D(3, 1, padding=\"same\", activation=\"sigmoid\")(up_conv4)\n",
    "\n",
    "    # unet model with Keras Functional API\n",
    "    u_net = tf.keras.Model(inputs, segmentation_map, name=\"U-Net\")\n",
    "\n",
    "    return u_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1af5a1d89a2d8dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:05.240718100Z",
     "start_time": "2023-11-30T09:26:03.195408300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Annmariya.sebastian\\PycharmProjects\\calcium-gui\\venv\\lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Annmariya.sebastian\\PycharmProjects\\calcium-gui\\venv\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "Model: \"U-Net\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 128, 128, 64)         1792      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 128, 128, 64)         36928     ['conv2d[0][0]']              \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 64, 64, 64)           0         ['conv2d_1[0][0]']            \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 64, 64, 64)           0         ['max_pooling2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 64, 64, 128)          73856     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 64, 64, 128)          147584    ['conv2d_2[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPoolin  (None, 32, 32, 128)          0         ['conv2d_3[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 32, 32, 128)          0         ['max_pooling2d_1[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 256)          295168    ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 32, 32, 256)          590080    ['conv2d_4[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPoolin  (None, 16, 16, 256)          0         ['conv2d_5[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 16, 16, 256)          0         ['max_pooling2d_2[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 512)          1180160   ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 16, 16, 512)          2359808   ['conv2d_6[0][0]']            \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPoolin  (None, 8, 8, 512)            0         ['conv2d_7[0][0]']            \n",
      " g2D)                                                                                             \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 8, 8, 512)            0         ['max_pooling2d_3[0][0]']     \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 8, 8, 1024)           4719616   ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 8, 8, 1024)           9438208   ['conv2d_8[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_transpose (Conv2DTr  (None, 16, 16, 512)          4719104   ['conv2d_9[0][0]']            \n",
      " anspose)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)   (None, 16, 16, 1024)         0         ['conv2d_transpose[0][0]',    \n",
      "                                                                     'conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, 16, 16, 1024)         0         ['concatenate[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 16, 16, 512)          4719104   ['dropout_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 16, 16, 512)          2359808   ['conv2d_10[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_1 (Conv2D  (None, 32, 32, 256)          1179904   ['conv2d_11[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate  (None, 32, 32, 512)          0         ['conv2d_transpose_1[0][0]',  \n",
      " )                                                                   'conv2d_5[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, 32, 32, 512)          0         ['concatenate_1[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 32, 32, 256)          1179904   ['dropout_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 256)          590080    ['conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_2 (Conv2D  (None, 64, 64, 128)          295040    ['conv2d_13[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 64, 64, 256)          0         ['conv2d_transpose_2[0][0]',  \n",
      " )                                                                   'conv2d_3[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, 64, 64, 256)          0         ['concatenate_2[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 64, 64, 128)          295040    ['dropout_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 64, 64, 128)          147584    ['conv2d_14[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_transpose_3 (Conv2D  (None, 128, 128, 64)         73792     ['conv2d_15[0][0]']           \n",
      " Transpose)                                                                                       \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate  (None, 128, 128, 128)        0         ['conv2d_transpose_3[0][0]',  \n",
      " )                                                                   'conv2d_1[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)         (None, 128, 128, 128)        0         ['concatenate_3[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 128, 128, 64)         73792     ['dropout_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 128, 128, 64)         36928     ['conv2d_16[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 128, 128, 3)          195       ['conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 34513475 (131.66 MB)\n",
      "Trainable params: 34513475 (131.66 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet_model = build_unet_model()\n",
    "unet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f986c12a18255df9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:05.256381100Z",
     "start_time": "2023-11-30T09:26:05.209474200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(unet_model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b715dd8ecc5e24f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Metrics to Evaluate your Semantic Segmentation Model\n",
    "\n",
    "1. Pixel Accuracy\n",
    "2. Intersection-Over-Union (Jaccard Index)\n",
    "3. Dice Coefficient (F1 Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8305aec75a2a843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T09:26:05.271994700Z",
     "start_time": "2023-11-30T09:26:05.209474200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def mean_iou(y_true, y_pred):\n",
    "    intersection = K.sum(K.abs(y_true * K.round(y_pred)))\n",
    "    union = K.sum(y_true) + K.sum(K.round(y_pred)) - intersection\n",
    "    iou = intersection / (union + K.epsilon())\n",
    "    return iou\n",
    "\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    numerator = 2 * K.sum(y_true * y_pred)\n",
    "    denominator = K.sum(y_true) + K.sum(y_pred)\n",
    "    dice = numerator / (denominator + K.epsilon())\n",
    "    return dice\n",
    "\n",
    "\n",
    "# number of pixels that are classified correctly in the generated segmentation mask\n",
    "def pixel_wise_accuracy(y_true, y_pred):\n",
    "    return K.mean(K.equal(K.round(y_true), K.round(y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d325b7463236a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T09:26:05.225096500Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Annmariya.sebastian\\PycharmProjects\\calcium-gui\\venv\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From C:\\Users\\Annmariya.sebastian\\PycharmProjects\\calcium-gui\\venv\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Annmariya.sebastian\\PycharmProjects\\calcium-gui\\venv\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "80/80 [==============================] - 1315s 16s/step - loss: 0.2693 - mean_iou: 0.5284 - dice_coefficient: 0.5838 - pixel_wise_accuracy: 0.9350 - val_loss: 0.0223 - val_mean_iou: 0.9468 - val_dice_coefficient: 0.9299 - val_pixel_wise_accuracy: 0.9942\n",
      "Epoch 2/5\n",
      "80/80 [==============================] - 1278s 16s/step - loss: 0.0117 - mean_iou: 0.9633 - dice_coefficient: 0.9697 - pixel_wise_accuracy: 0.9970 - val_loss: 0.0075 - val_mean_iou: 0.9723 - val_dice_coefficient: 0.9801 - val_pixel_wise_accuracy: 0.9985\n",
      "Epoch 3/5\n",
      "80/80 [==============================] - 1269s 16s/step - loss: 0.0075 - mean_iou: 0.9721 - dice_coefficient: 0.9801 - pixel_wise_accuracy: 0.9984 - val_loss: 0.0069 - val_mean_iou: 0.9737 - val_dice_coefficient: 0.9815 - val_pixel_wise_accuracy: 0.9987\n",
      "Epoch 4/5\n",
      "80/80 [==============================] - 1223s 15s/step - loss: 0.0069 - mean_iou: 0.9735 - dice_coefficient: 0.9812 - pixel_wise_accuracy: 0.9986 - val_loss: 0.0068 - val_mean_iou: 0.9738 - val_dice_coefficient: 0.9818 - val_pixel_wise_accuracy: 0.9987\n",
      "Epoch 5/5\n",
      "55/80 [===================>..........] - ETA: 43:46 - loss: 0.0067 - mean_iou: 0.9741 - dice_coefficient: 0.9817 - pixel_wise_accuracy: 0.9988"
     ]
    }
   ],
   "source": [
    "unet_model.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=[mean_iou, dice_coefficient, pixel_wise_accuracy])\n",
    "history = unet_model.fit(train_images, train_mask_images, epochs=5, batch_size=32, validation_data=(valid_images, valid_mask_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c9744ae430908d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d06b7a3595563",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = unet_model.evaluate(test_images, test_mask_images)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c36f2c51097baa",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['mean_iou'], label='Training Mean IoU')\n",
    "plt.plot(history.history['val_mean_iou'], label='Validation Mean IoU')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc1d16f7354be8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Sample prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eadb97e2b9cf3b8",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "sample_index = 0  # Change this index to visualize predictions on different test images\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Original test image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(test_images[sample_index])\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "# Ground truth mask\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(test_mask_images[sample_index])\n",
    "plt.title('Ground Truth Mask')\n",
    "plt.axis('off')\n",
    "\n",
    "# Model's predicted mask\n",
    "predictions = unet_model.predict(test_images[sample_index][np.newaxis, ...])\n",
    "binary_mask = (predictions[0, :, :, 0] > 0.5).astype(np.uint8)  # Thresholding for a binary mask\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(binary_mask, cmap='gray')  # Displaying as a grayscale image\n",
    "plt.title('Predicted Mask')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b222112ae36601f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Reuse of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f3b19dbb6fcd64",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "loaded_model = load_model('saved_unet.h5')\n",
    "\n",
    "predictions = loaded_model.predict(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
